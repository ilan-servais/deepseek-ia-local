# ğŸ¤– RAG DeepSeek Chat Interface

Une interface moderne pour interagir avec le modÃ¨le deepseek-r1:1.5b via Ollama, construite avec Next.js 15 et TailwindCSS.

![Interface Screenshot](https://placehold.co/600x400?text=DeepSeek+Chat+Interface)

## ğŸ¯ Contexte gÃ©nÃ©ral

Ce projet est une interface de chatbot dÃ©veloppÃ©e avec:
- **Next.js 15** (App Router)
- **TypeScript**
- **TailwindCSS**
- **Vercel AI SDK** (`useChat`)

Le backend fonctionne via une API personnalisÃ©e `/api/chat`, connectÃ©e localement Ã  **Ollama** (http://localhost:11434).
Le modÃ¨le utilisÃ© est **`deepseek-r1:1.5b`**, accessible via l'API REST d'Ollama avec support complet du streaming des rÃ©ponses.

## ğŸš€ FonctionnalitÃ©s principales

- **Interface moderne** inspirÃ©e de ChatGPT
- **Streaming de texte** en temps rÃ©el
- **Design responsive** avec TailwindCSS
- **ExpÃ©rience utilisateur intuitive**:
  - Messages utilisateur Ã  droite (bleu clair)
  - Messages IA Ã  gauche (gris clair)
  - Indicateurs visuels de chargement
- **Raccourcis clavier**:
  - `Enter` : envoyer le message
  - `Shift + Enter` : retour Ã  la ligne
- **Base prÃªte pour RAG** avec conteneur Docker pour PostgreSQL + pgvector
- **Architecture modulaire** avec composants rÃ©utilisables

## ğŸ› ï¸ Installation & DÃ©marrage

### PrÃ©requis
- Node.js 18+ et npm
- [Docker](https://www.docker.com/) pour le conteneur PostgreSQL
- [Ollama](https://ollama.ai/) installÃ© localement
- ModÃ¨le deepseek-r1:1.5b installÃ© sur Ollama

### Installation

```bash
# Cloner le dÃ©pÃ´t
git clone <repo>

# AccÃ©der au rÃ©pertoire
cd rag-deepseek

# Installer les dÃ©pendances
npm install

# Configurer les variables d'environnement
cp .env.example .env.local
```

### Configuration de la base de donnÃ©es

```bash
# DÃ©marrer le conteneur PostgreSQL avec pgvector
docker-compose up -d

# VÃ©rifier que le conteneur est bien dÃ©marrÃ©
docker ps
```

### Configuration d'Ollama

```bash
# TÃ©lÃ©charger le modÃ¨le deepseek-r1:1.5b
ollama pull deepseek-r1:1.5b

# VÃ©rifier que Ollama est en cours d'exÃ©cution
ollama list
```

### Lancement du projet

```bash
# DÃ©marrer le serveur de dÃ©veloppement
npm run dev
```

Ouvrez [http://localhost:3000](http://localhost:3000) dans votre navigateur pour voir l'interface.

## ğŸ“„ Types de documents supportÃ©s

Le systÃ¨me RAG peut traiter les formats de fichiers suivants :

1. **PDF** (.pdf)
   - Documents textuels
   - Rapports, articles, livres
   - CV et documents professionnels

2. **Texte brut** (.txt)
   - Fichiers texte simple
   - Notes, transcriptions
   - DonnÃ©es structurÃ©es en texte

3. **Markdown** (.md)
   - Documentation technique
   - Notes formatÃ©es
   - Articles avec mise en forme lÃ©gÃ¨re

Le systÃ¨me extrait automatiquement le texte de ces documents, les dÃ©coupe en segments plus petits ("chunks") et gÃ©nÃ¨re des embeddings vectoriels pour permettre la recherche sÃ©mantique.

**Limites actuelles :**
- Taille maximale de fichier : 10MB
- Les images dans les PDFs ne sont pas analysÃ©es (extraction de texte uniquement)
- Les tableaux complexes peuvent perdre leur structure lors de l'extraction

## ğŸ”§ Configuration

Assurez-vous que votre fichier `.env.local` contient les variables d'environnement suivantes:

```
OLLAMA_API_HOST=http://localhost:11434

# Configuration PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=postgres
```

## ğŸ” DÃ©pannage

Si vous rencontrez des erreurs:

1. VÃ©rifiez qu'Ollama est bien lancÃ© et que le modÃ¨le est installÃ©:
   ```bash
   ollama list
   ```

2. VÃ©rifiez que le conteneur Docker est en cours d'exÃ©cution:
   ```bash
   docker ps
   ```

3. Si vous rencontrez des erreurs d'authentification avec PostgreSQL:
   ```bash
   # ArrÃªter le conteneur existant
   docker-compose down
   
   # Supprimer le volume (ATTENTION: cette opÃ©ration supprimera dÃ©finitivement toutes les donnÃ©es 
   # stockÃ©es dans votre base PostgreSQL, y compris les documents et embeddings gÃ©nÃ©rÃ©s)
   docker volume rm rag-deepseek_pg_data
   
   # RedÃ©marrer le conteneur
   docker-compose up -d
   ```

4. Initialisez la base de donnÃ©es depuis l'interface web:
   - Allez dans l'onglet "Documents"
   - Cliquez sur "Configurer la Base de DonnÃ©es"
   - Suivez les Ã©tapes pour vÃ©rifier la connexion et initialiser les tables

5. Pour des diagnostics, consultez la page `/api/diagnostics` dans votre navigateur.

## ğŸ“ Structure du projet

```txt
rag-deepseek/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ chat/
â”‚   â”‚       â””â”€â”€ route.ts      # Endpoint API pour le chat
â”‚   â”œâ”€â”€ globals.css           # Styles globaux
â”‚   â”œâ”€â”€ layout.tsx            # Layout principal
â”‚   â””â”€â”€ page.tsx              # Page d'accueil
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ Button.tsx        # Composant de bouton rÃ©utilisable
â”‚   â”‚   â”œâ”€â”€ Form.tsx          # Composant de formulaire
â”‚   â”‚   â””â”€â”€ Input.tsx         # Composant d'input 
â”‚   â””â”€â”€ chat/
â”‚       â””â”€â”€ Client.tsx        # Interface principale du chat
â”œâ”€â”€ public/
â””â”€â”€ ... (fichiers de configuration)
```

## ğŸ” Architecture technique

### Frontend
- **Next.js 15** avec App Router pour le routage et le rendu
- **TailwindCSS** pour le style
- **Vercel AI SDK** pour la gestion des conversations et du streaming
- **Composants React** modulaires et rÃ©utilisables

### Backend
- **API Route Next.js** pour l'endpoint `/api/chat`
- **Streaming** des rÃ©ponses du modÃ¨le en temps rÃ©el
- **IntÃ©gration Ollama** via son API REST
- **Transformation des donnÃ©es** pour compatibilitÃ© avec le SDK Vercel AI

### ModÃ¨le IA
- **deepseek-r1:1.5b** - Un modÃ¨le de langage efficace et lÃ©ger

