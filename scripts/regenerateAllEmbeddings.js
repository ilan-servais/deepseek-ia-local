const { Pool } = require('pg');
const path = require('path');
const dotenv = require('dotenv');

// Charger les variables d'environnement depuis .env.local
dotenv.config({ path: path.join(__dirname, '..', '.env.local') });

// Configuration de la connexion √† la base de donn√©es
const pool = new Pool({
  user: process.env.POSTGRES_USER || 'postgres',
  host: process.env.POSTGRES_HOST || 'localhost',
  database: process.env.POSTGRES_DB || 'postgres',
  password: process.env.POSTGRES_PASSWORD || 'postgres',
  port: parseInt(process.env.POSTGRES_PORT || '5432'),
});

// Fonction de chunking s√©mantique am√©lior√©e
function chunkTextSemantic(text, options = {}) {
  const { minSize = 200, targetSize = 500, maxSize = 800 } = options;
  
  // Nettoyer le texte
  const cleanedText = text
    .replace(/\r\n/g, '\n')
    .replace(/\t/g, ' ')
    .replace(/\n{3,}/g, '\n\n');
  
  // 1. D√©couper par sections (titres, sous-titres)
  const sectionPattern = /(?:^|\n)(#{1,3} .+?)(?:\n|$)/g;
  const sections = [];
  let lastIndex = 0;
  let match;
  
  // Extraire les sections avec leurs ent√™tes
  while ((match = sectionPattern.exec(cleanedText)) !== null) {
    const heading = match[1];
    const startIdx = match.index;
    
    if (startIdx > lastIndex) {
      // Ajouter le texte pr√©c√©dent comme une section sans titre
      sections.push({
        heading: '',
        text: cleanedText.substring(lastIndex, startIdx)
      });
    }
    
    // Trouver la fin de la section (prochain titre ou fin de texte)
    const nextMatch = sectionPattern.exec(cleanedText);
    sectionPattern.lastIndex = match.index + match[0].length; // Reset pour revenir au bon endroit
    
    const endIdx = nextMatch ? nextMatch.index : cleanedText.length;
    
    sections.push({
      heading,
      text: cleanedText.substring(startIdx, endIdx)
    });
    
    lastIndex = endIdx;
  }
  
  // Ajouter le reste du texte s'il y en a
  if (lastIndex < cleanedText.length) {
    sections.push({
      heading: '',
      text: cleanedText.substring(lastIndex)
    });
  }
  
  // 2. Pour chaque section, d√©couper en paragraphes
  const chunks = [];
  
  for (const section of sections) {
    // Sauter les sections vides
    if (!section.text.trim()) continue;
    
    // D√©couper en paragraphes
    const paragraphs = section.text.split(/\n{2,}/);
    let currentChunk = section.heading ? section.heading + '\n\n' : '';
    
    for (const paragraph of paragraphs) {
      const trimmedPara = paragraph.trim();
      if (!trimmedPara) continue;
      
      const paraWithNewline = currentChunk.length > 0 ? '\n\n' + trimmedPara : trimmedPara;
      
      // Si ajouter ce paragraphe d√©passe la taille maximale, on sauvegarde le chunk actuel
      if (currentChunk.length > 0 && 
          (currentChunk.length + paraWithNewline.length) > maxSize && 
          currentChunk.length >= minSize) {
        
        chunks.push(currentChunk);
        
        // Commencer un nouveau chunk, potentiellement avec un chevauchement
        if (section.heading) {
          currentChunk = section.heading + '\n\n' + trimmedPara;
        } else {
          currentChunk = trimmedPara;
        }
      } else {
        // Ajouter le paragraphe au chunk actuel
        currentChunk += paraWithNewline;
      }
    }
    
    // Si apr√®s avoir trait√© tous les paragraphes il reste du contenu, l'ajouter comme chunk
    if (currentChunk.trim().length > 0) {
      chunks.push(currentChunk);
    }
  }
  
  return chunks;
}

// Fonction pour formater les embeddings pour pgvector
function formatEmbeddingForPgVector(embedding) {
  return `[${embedding.join(',')}]`;
}

async function regenerateAllEmbeddings() {
  const client = await pool.connect();
  try {
    console.log("üöÄ D√©marrage de la r√©g√©n√©ration compl√®te des chunks et embeddings...");
    
    // V√©rifier si la colonne model_name existe dans la table embeddings
    let hasModelNameColumn = false;
    try {
      const columnCheck = await client.query(`
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_name = 'embeddings' AND column_name = 'model_name'
      `);
      hasModelNameColumn = columnCheck.rows.length > 0;
      console.log(`V√©rification de la colonne model_name: ${hasModelNameColumn ? 'pr√©sente' : 'absente'}`);
      
      // Si la colonne n'existe pas, l'ajouter
      if (!hasModelNameColumn) {
        console.log("üîß Ajout de la colonne model_name √† la table embeddings...");
        await client.query(`
          ALTER TABLE embeddings 
          ADD COLUMN IF NOT EXISTS model_name TEXT DEFAULT 'unknown'
        `);
        console.log("‚úÖ Colonne model_name ajout√©e avec succ√®s");
        hasModelNameColumn = true;
      }
    } catch (error) {
      console.error("‚ùå Erreur lors de la v√©rification de la structure de la table:", error);
      hasModelNameColumn = false; // En cas d'erreur, supposer que la colonne n'existe pas
    }
    
    // 1. R√©cup√©rer tous les documents
    const documents = await client.query(`
      SELECT id, filename, content, title, author, category, date, source_url
      FROM documents 
      ORDER BY id
    `);
    
    console.log(`üìö Trouv√© ${documents.rows.length} documents √† retraiter`);
    
    let totalChunks = 0;
    let successfulEmbeddings = 0;
    let failedEmbeddings = 0;
    
    // 2. Pour chaque document, supprimer les chunks existants et recr√©er
    for (const doc of documents.rows) {
      console.log(`\nüìÑ Traitement du document "${doc.filename}" (ID: ${doc.id})...`);
      
      // Supprimer les chunks existants (les embeddings seront supprim√©s en cascade)
      await client.query('DELETE FROM chunks WHERE document_id = $1', [doc.id]);
      console.log(`üóëÔ∏è Chunks et embeddings existants supprim√©s pour le document ${doc.id}`);
      
      // D√©couper le texte en chunks s√©mantiques
      const chunks = chunkTextSemantic(doc.content, {
        minSize: 200,
        targetSize: 500,
        maxSize: 800,
        overlapSize: 50
      });
      
      console.log(`‚úÇÔ∏è Texte red√©coup√© en ${chunks.length} chunks s√©mantiques`);
      totalChunks += chunks.length;
      
      // V√©rifier que Ollama est disponible
      try {
        const ollamaCheck = await fetch(`${process.env.OLLAMA_API_HOST}/api/tags`);
        if (!ollamaCheck.ok) {
          throw new Error(`Ollama API n'est pas disponible. Code: ${ollamaCheck.status}`);
        }
        console.log('‚úÖ Connexion √† Ollama v√©rifi√©e avec succ√®s');
      } catch (error) {
        console.error('‚ùå Erreur lors de la v√©rification de Ollama:', error);
        throw new Error('Impossible de se connecter √† Ollama. Assurez-vous que le service est en cours d\'ex√©cution.');
      }
      
      // Traiter chaque chunk
      for (let i = 0; i < chunks.length; i++) {
        console.log(`üß© Traitement du chunk ${i+1}/${chunks.length} (taille: ${chunks[i].length} caract√®res)...`);
        
        // Ins√©rer le chunk
        const chunkResult = await client.query(
          'INSERT INTO chunks(document_id, content, chunk_index) VALUES($1, $2, $3) RETURNING id',
          [doc.id, chunks[i], i]
        );
        
        const chunkId = chunkResult.rows[0].id;
        
        try {
          // Limiter la taille du texte pour √©viter les erreurs
          const maxTextLength = 8000;
          const truncatedText = chunks[i].length > maxTextLength 
            ? chunks[i].substring(0, maxTextLength) 
            : chunks[i];
          
          // G√©n√©rer l'embedding pour ce chunk
          const embeddingResponse = await fetch(`${process.env.OLLAMA_API_HOST}/api/embeddings`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              model: 'deepseek-r1:1.5b',
              prompt: truncatedText,
            })
          });
          
          if (!embeddingResponse.ok) {
            const errorText = await embeddingResponse.text();
            throw new Error(`Erreur lors de la g√©n√©ration de l'embedding: ${errorText}`);
          }
          
          const embeddingData = await embeddingResponse.json();
          
          if (!embeddingData.embedding || !Array.isArray(embeddingData.embedding)) {
            throw new Error(`Format d'embedding invalide: ${JSON.stringify(embeddingData)}`);
          }
          
          const embedding = embeddingData.embedding;
          console.log(`üß† Embedding g√©n√©r√© avec succ√®s: ${embedding.length} dimensions`);
          
          // Ins√©rer l'embedding dans la base de donn√©es - Adapter la requ√™te en fonction de la pr√©sence de model_name
          const formattedEmbedding = formatEmbeddingForPgVector(embedding);
          
          if (hasModelNameColumn) {
            await client.query(
              'INSERT INTO embeddings (chunk_id, embedding, model_name) VALUES ($1, $2, $3)',
              [chunkId, formattedEmbedding, 'deepseek-r1:1.5b']
            );
          } else {
            await client.query(
              'INSERT INTO embeddings (chunk_id, embedding) VALUES ($1, $2)',
              [chunkId, formattedEmbedding]
            );
          }
          
          console.log(`üíæ Embedding ins√©r√© avec succ√®s pour le chunk ${i+1}`);
          successfulEmbeddings++;
          
        } catch (embeddingError) {
          console.error(`‚ùå Erreur lors de la g√©n√©ration de l'embedding pour le chunk ${i}:`, embeddingError);
          failedEmbeddings++;
        }
        
        // Petite pause pour √©viter de surcharger l'API
        await new Promise(resolve => setTimeout(resolve, 200));
      }
    }
    
    console.log(`\n===== üìä R√©capitulatif =====`);
    console.log(`üìö Documents trait√©s: ${documents.rows.length}`);
    console.log(`üß© Total chunks cr√©√©s: ${totalChunks}`);
    console.log(`‚úÖ Embeddings r√©ussis: ${successfulEmbeddings}`);
    console.log(`‚ùå Embeddings √©chou√©s: ${failedEmbeddings}`);
    
  } catch (error) {
    console.error("‚ùå Erreur lors de la r√©g√©n√©ration des embeddings:", error);
  } finally {
    client.release();
    await pool.end();
  }
}

// Ex√©cuter la r√©g√©n√©ration
regenerateAllEmbeddings().then(() => {
  console.log('üéâ R√©g√©n√©ration termin√©e !');
}).catch(error => {
  console.error('‚ùå Erreur fatale:', error);
}).finally(() => {
  process.exit(0);
});
